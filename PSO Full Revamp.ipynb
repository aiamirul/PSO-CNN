{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.layers import AveragePooling2D,Dropout,Flatten,Dense,Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from imutils import paths\n",
    "from random import shuffle, choice\n",
    "from PIL import Image\n",
    "from random import randint\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "d = datetime.utcnow()\n",
    "unixtime = calendar.timegm(d.utctimetuple())\n",
    "os.mkdir(\"EXPERIMENT-\"+str(unixtime))\n",
    "path = \"EXPERIMENT-\"+str(unixtime)+\"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading (Fake data for Now ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(100, 28, 28), y=(100,)\n",
      "Test: X=(10, 28, 28), y=(10,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 224, 224, 3)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of loading the mnist dataset\n",
    "from keras.datasets import mnist\n",
    "from matplotlib import pyplot\n",
    "# load dataset\n",
    "(train_data, train_target), (test_data, test_target) = mnist.load_data()\n",
    "# summarize loaded dataset\n",
    "                          \n",
    "train_data = train_data[0:100]                           \n",
    "train_target = train_target[0:100]   \n",
    "test_data = test_data[0:10]   \n",
    "test_target = test_target[0:10]   \n",
    "\n",
    "print('Train: X=%s, y=%s' % (train_data.shape, train_target.shape))\n",
    "print('Test: X=%s, y=%s' % (test_data.shape, test_target.shape))\n",
    "  \n",
    "#Make 0-5 a group as 0  and 5+ as 1 for simplicity\n",
    "test_target = np.where(test_target < 5, 0, 1)  \n",
    "train_target = np.where(train_target < 5, 0, 1)  \n",
    "\n",
    "train_data = np.array(train_data.reshape((train_data.shape[0], 28, 28, 1)))\n",
    "test_data = np.array(test_data.reshape((test_data.shape[0], 28, 28, 1)))\n",
    "\n",
    "train_data =(train_data.repeat(3, -1))  # repeat the last (-1) dimension three times\n",
    "test_data = (test_data.repeat(3, -1))  # repeat the last (-1) dimension three times\n",
    "\n",
    "train_data =(train_data.repeat(8, -2))  # repeat the last (-1) dimension three times\n",
    "test_data = (test_data.repeat(8, -2))  # repeat the last (-1) dimension three times\n",
    "\n",
    "\n",
    "train_data =(train_data.repeat(8, -3))  # repeat the last (-1) dimension three times\n",
    "test_data = (test_data.repeat(8, -3))  # repeat the last (-1) dimension three times\n",
    "\n",
    "tuple(train_target).count(1)\n",
    "tuple(train_target).count(0)\n",
    "\n",
    "\n",
    "tuple(test_target).count(1)\n",
    "tuple(test_target).count(0)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunModelDesign(settings):\n",
    "    \n",
    "  \n",
    "    #Load the VGG19 model, top layers off\n",
    "    baseModel = VGG19(weights=\"imagenet\",include_top=False,input_tensor=Input(shape=(224,224,3)))\n",
    "    INIT_LR= 1e-2 # 0.001 \n",
    "    EPOCHS= 10\n",
    "    BS= 8\n",
    "    \n",
    "    print(\"Running Experiment : \",settings)\n",
    "    print(\"layer-1:\",settings[0])\n",
    "    headModel=baseModel.output\n",
    "    headModel=AveragePooling2D(pool_size=(4,4))(headModel)\n",
    "    headModel=Flatten(name=\"flatten\")(headModel)\n",
    "    headModel=Dense(settings[0],activation=\"relu\")(headModel) #LAYER \n",
    "    headModel=Dense(settings[1],activation=\"relu\")(headModel) #LAYER \n",
    "    headModel=Dense(settings[2],activation=\"relu\")(headModel) #LAYER \n",
    "    headModel=Dense(settings[3],activation=\"relu\")(headModel) #LAYER \n",
    "    headModel=Dropout(settings[4])(headModel)# MIN-MAX 0.1-0.5\n",
    "    headModel=Dense(2,activation=\"softmax\")(headModel) # abnormal /non compliant \n",
    "    model= Model(inputs=baseModel.input, outputs=headModel)\n",
    "    \n",
    "    for layer in baseModel.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    if(debug):\n",
    "        model.summary()\n",
    "        \n",
    "    \n",
    "\n",
    "    print(\"[INFO] compiling model....\")\n",
    "    opt=Adam(lr=INIT_LR,decay=INIT_LR/EPOCHS)\n",
    "    model.compile(loss=\"binary_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])\n",
    "    \n",
    "    print(\"[INFO] Training the model....\")\n",
    "    \n",
    "    \n",
    "    filepath = path+str(int(settings[0]))+\"-\"+str(int(settings[1]))+\"-\"+str(int(settings[2]))+\"-\"+str(int(settings[3]))+\"-DP0\"+str(int(10*settings[4]))+\"-VGG19.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, \n",
    "                                 save_best_only=True, mode='max')\n",
    "    print(\"[INFO] model file: \",filepath)\n",
    "    \n",
    "    \n",
    "    #initialize training data augmentation \n",
    "    trainAug= ImageDataGenerator(rotation_range=15,\n",
    "                                 rescale=1./255,\n",
    "                                 width_shift_range=0.2,\n",
    "                                 height_shift_range=0.2,\n",
    "                                 shear_range=0.2,\n",
    "                                 zoom_range=0.2,\n",
    "                                 horizontal_flip=True,\n",
    "                                 fill_mode=\"nearest\")\n",
    "    \n",
    "\n",
    "    History=model.fit(\n",
    "        trainAug.flow(train_data,train_target,batch_size=BS),\n",
    "        steps_per_epoch=len(train_data)//BS,\n",
    "        validation_data=(test_data,test_target),\n",
    "        validation_steps=len(test_data)//BS,\n",
    "        epochs=1,\n",
    "        callbacks=[checkpoint])\n",
    "    \n",
    "    num_val_samples = len(test_data)\n",
    "    val_batch_size = BS\n",
    "\n",
    "    val_steps = np.ceil(num_val_samples / val_batch_size) #160 / 8 = 20 steps per epoch\n",
    "    print(len(test_data))\n",
    "\n",
    "        # Here the best epoch will be used.\n",
    "    #validate the model\n",
    "    print(\"[INFO] evaluating model....\")\n",
    "    model.save(filepath)\n",
    "    model.load_weights(filepath)\n",
    "    val_loss, val_acc = model.evaluate(test_data,test_target)\n",
    "    print(test_target)\n",
    "    print('val_loss:', val_loss)\n",
    "    print('val_acc:', val_acc)\n",
    "    \n",
    "    \n",
    "    results = [val_loss,val_acc]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testrun 1 Population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results = RunModelDesign(popdata[i])\n",
    "print(\"Validation Loss \",results[0])\n",
    "print(\"Validation Acc \",results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSO LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popualtion Shape :  (5, 11)\n",
      "[24.   48.   24.   48.    0.4   0.2   0.82  0.9   1.    0.    0.  ]\n",
      "Results Acc: 0.0\n",
      "Results Loss: 1.0\n",
      "Population Results Saved :  EXPERIMENT-1616702854/PopualtionData_5_TIME1616702856.csv\n",
      "[[32.   24.   40.   24.    0.4   0.63  0.06  0.9   1.    0.    0.  ]\n",
      " [24.   48.   24.   48.    0.4   0.2   0.82  0.9   1.    0.    0.  ]\n",
      " [16.    8.   56.   48.    0.4   0.36  0.87  0.9   1.    0.    0.  ]\n",
      " [16.   48.   24.   40.    0.2   0.37  0.74  0.9   1.    0.    0.  ]\n",
      " [56.   24.   64.   16.    0.2   0.62  0.15  0.9   1.    0.    0.  ]]\n",
      "Running Experiment :  [32.   24.   40.   24.    0.4   0.63  0.06  0.9   1.    0.    0.  ]\n",
      "layer-1: 32.0\n",
      "[INFO] compiling model....\n",
      "[INFO] Training the model....\n",
      "[INFO] model file:  EXPERIMENT-1616702854/32-24-40-24-DP04-VGG19.h5\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6961 - accuracy: 0.5000\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.60000, saving model to EXPERIMENT-1616702854/32-24-40-24-DP04-VGG19.h5\n",
      "12/12 [==============================] - 9s 756ms/step - loss: 0.6961 - accuracy: 0.5000 - val_loss: 0.7257 - val_accuracy: 0.6000\n",
      "10\n",
      "[INFO] evaluating model....\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.7257 - accuracy: 0.6000\n",
      "[1 0 0 0 0 0 0 1 1 1]\n",
      "val_loss: 0.7256500124931335\n",
      "val_acc: 0.6000000238418579\n",
      "Validation Loss  0.7256500124931335\n",
      "Validation Acc  0.6000000238418579\n",
      "[[32.   24.   40.   24.    0.4   0.63  0.06  0.9   0.73  0.6   0.  ]\n",
      " [24.   48.   24.   48.    0.4   0.2   0.82  0.9   1.    0.    0.  ]\n",
      " [16.    8.   56.   48.    0.4   0.36  0.87  0.9   1.    0.    0.  ]\n",
      " [16.   48.   24.   40.    0.2   0.37  0.74  0.9   1.    0.    0.  ]\n",
      " [56.   24.   64.   16.    0.2   0.62  0.15  0.9   1.    0.    0.  ]]\n",
      "Saved Progress [32.   24.   40.   24.    0.4   0.63  0.06  0.9   0.73  0.6   0.  ]\n",
      "Running Experiment :  [24.   48.   24.   48.    0.4   0.2   0.82  0.9   1.    0.    0.  ]\n",
      "layer-1: 24.0\n",
      "[INFO] compiling model....\n",
      "[INFO] Training the model....\n",
      "[INFO] model file:  EXPERIMENT-1616702854/24-48-24-48-DP04-VGG19.h5\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6980 - accuracy: 0.4891\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.70000, saving model to EXPERIMENT-1616702854/24-48-24-48-DP04-VGG19.h5\n",
      "12/12 [==============================] - 9s 763ms/step - loss: 0.6980 - accuracy: 0.4891 - val_loss: 0.7004 - val_accuracy: 0.7000\n",
      "10\n",
      "[INFO] evaluating model....\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7004 - accuracy: 0.7000\n",
      "[1 0 0 0 0 0 0 1 1 1]\n",
      "val_loss: 0.7003573179244995\n",
      "val_acc: 0.699999988079071\n",
      "Validation Loss  0.7003573179244995\n",
      "Validation Acc  0.699999988079071\n",
      "[[32.   24.   40.   24.    0.4   0.63  0.06  0.9   0.73  0.6   0.  ]\n",
      " [24.   48.   24.   48.    0.4   0.2   0.82  0.9   0.7   0.7   0.  ]\n",
      " [16.    8.   56.   48.    0.4   0.36  0.87  0.9   1.    0.    0.  ]\n",
      " [16.   48.   24.   40.    0.2   0.37  0.74  0.9   1.    0.    0.  ]\n",
      " [56.   24.   64.   16.    0.2   0.62  0.15  0.9   1.    0.    0.  ]]\n",
      "Saved Progress [24.   48.   24.   48.    0.4   0.2   0.82  0.9   0.7   0.7   0.  ]\n",
      "Running Experiment :  [16.    8.   56.   48.    0.4   0.36  0.87  0.9   1.    0.    0.  ]\n",
      "layer-1: 16.0\n",
      "[INFO] compiling model....\n",
      "[INFO] Training the model....\n",
      "[INFO] model file:  EXPERIMENT-1616702854/16-8-56-48-DP04-VGG19.h5\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.5000\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.40000, saving model to EXPERIMENT-1616702854/16-8-56-48-DP04-VGG19.h5\n",
      "12/12 [==============================] - 9s 760ms/step - loss: 0.6941 - accuracy: 0.5000 - val_loss: 0.6957 - val_accuracy: 0.4000\n",
      "10\n",
      "[INFO] evaluating model....\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6957 - accuracy: 0.4000\n",
      "[1 0 0 0 0 0 0 1 1 1]\n",
      "val_loss: 0.6957405209541321\n",
      "val_acc: 0.4000000059604645\n",
      "Validation Loss  0.6957405209541321\n",
      "Validation Acc  0.4000000059604645\n",
      "[[32.   24.   40.   24.    0.4   0.63  0.06  0.9   0.73  0.6   0.  ]\n",
      " [24.   48.   24.   48.    0.4   0.2   0.82  0.9   0.7   0.7   0.  ]\n",
      " [16.    8.   56.   48.    0.4   0.36  0.87  0.9   0.7   0.4   0.  ]\n",
      " [16.   48.   24.   40.    0.2   0.37  0.74  0.9   1.    0.    0.  ]\n",
      " [56.   24.   64.   16.    0.2   0.62  0.15  0.9   1.    0.    0.  ]]\n",
      "Saved Progress [16.    8.   56.   48.    0.4   0.36  0.87  0.9   0.7   0.4   0.  ]\n",
      "Running Experiment :  [16.   48.   24.   40.    0.2   0.37  0.74  0.9   1.    0.    0.  ]\n",
      "layer-1: 16.0\n",
      "[INFO] compiling model....\n",
      "[INFO] Training the model....\n",
      "[INFO] model file:  EXPERIMENT-1616702854/16-48-24-40-DP02-VGG19.h5\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7038 - accuracy: 0.5978\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.60000, saving model to EXPERIMENT-1616702854/16-48-24-40-DP02-VGG19.h5\n",
      "12/12 [==============================] - 9s 777ms/step - loss: 0.7038 - accuracy: 0.5978 - val_loss: 1.0494 - val_accuracy: 0.6000\n",
      "10\n",
      "[INFO] evaluating model....\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0494 - accuracy: 0.6000\n",
      "[1 0 0 0 0 0 0 1 1 1]\n",
      "val_loss: 1.0493688583374023\n",
      "val_acc: 0.6000000238418579\n",
      "Validation Loss  1.0493688583374023\n",
      "Validation Acc  0.6000000238418579\n",
      "[[32.   24.   40.   24.    0.4   0.63  0.06  0.9   0.73  0.6   0.  ]\n",
      " [24.   48.   24.   48.    0.4   0.2   0.82  0.9   0.7   0.7   0.  ]\n",
      " [16.    8.   56.   48.    0.4   0.36  0.87  0.9   0.7   0.4   0.  ]\n",
      " [16.   48.   24.   40.    0.2   0.37  0.74  0.9   1.05  0.6   0.  ]\n",
      " [56.   24.   64.   16.    0.2   0.62  0.15  0.9   1.    0.    0.  ]]\n",
      "Saved Progress [16.   48.   24.   40.    0.2   0.37  0.74  0.9   1.05  0.6   0.  ]\n",
      "Running Experiment :  [56.   24.   64.   16.    0.2   0.62  0.15  0.9   1.    0.    0.  ]\n",
      "layer-1: 56.0\n",
      "[INFO] compiling model....\n",
      "[INFO] Training the model....\n",
      "[INFO] model file:  EXPERIMENT-1616702854/56-24-64-16-DP02-VGG19.h5\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7053 - accuracy: 0.4891\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.40000, saving model to EXPERIMENT-1616702854/56-24-64-16-DP02-VGG19.h5\n",
      "12/12 [==============================] - 9s 783ms/step - loss: 0.7053 - accuracy: 0.4891 - val_loss: 0.6933 - val_accuracy: 0.4000\n",
      "10\n",
      "[INFO] evaluating model....\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6933 - accuracy: 0.4000\n",
      "[1 0 0 0 0 0 0 1 1 1]\n",
      "val_loss: 0.6933069229125977\n",
      "val_acc: 0.4000000059604645\n",
      "Validation Loss  0.6933069229125977\n",
      "Validation Acc  0.4000000059604645\n",
      "[[32.   24.   40.   24.    0.4   0.63  0.06  0.9   0.73  0.6   0.  ]\n",
      " [24.   48.   24.   48.    0.4   0.2   0.82  0.9   0.7   0.7   0.  ]\n",
      " [16.    8.   56.   48.    0.4   0.36  0.87  0.9   0.7   0.4   0.  ]\n",
      " [16.   48.   24.   40.    0.2   0.37  0.74  0.9   1.05  0.6   0.  ]\n",
      " [56.   24.   64.   16.    0.2   0.62  0.15  0.9   0.69  0.4   0.  ]]\n",
      "Saved Progress [56.   24.   64.   16.    0.2   0.62  0.15  0.9   0.69  0.4   0.  ]\n",
      "[[32.   24.   40.   24.    0.4   0.63  0.06  0.9   0.73  0.6   0.  ]\n",
      " [24.   48.   24.   48.    0.4   0.2   0.82  0.9   0.7   0.7   0.  ]\n",
      " [16.    8.   56.   48.    0.4   0.36  0.87  0.9   0.7   0.4   0.  ]\n",
      " [16.   48.   24.   40.    0.2   0.37  0.74  0.9   1.05  0.6   0.  ]\n",
      " [56.   24.   64.   16.    0.2   0.62  0.15  0.9   0.69  0.4   0.  ]]\n",
      "1 -Calculate Fitness : 1.95\n",
      "2 -Calculate Fitness : 2.2\n",
      "3 -Calculate Fitness : 1.9\n",
      "4 -Calculate Fitness : 0.35\n",
      "5 -Calculate Fitness : 1.95\n",
      "Best Model Found [24.   48.   24.   48.    0.4   0.2   0.82  0.9   0.7   0.7   2.2 ]\n",
      " L1     L2     L3    L4    DP    V      P     W    Loss   Acc   Fit\n",
      "[[32.   24.   40.   24.    0.4   0.63  0.06  0.9   0.73  0.6   1.95]\n",
      " [24.   48.   24.   48.    0.4   0.2   0.82  0.9   0.7   0.7   2.2 ]\n",
      " [16.    8.   56.   48.    0.4   0.36  0.87  0.9   0.7   0.4   1.9 ]\n",
      " [16.   48.   24.   40.    0.2   0.37  0.74  0.9   1.05  0.6   0.35]\n",
      " [56.   24.   64.   16.    0.2   0.62  0.15  0.9   0.69  0.4   1.95]]\n",
      "Updates New Position adn Velocity\n",
      " L1     L2     L3    L4    DP    V      P     W    Loss   Acc   Fit\n",
      "[[32.   24.   40.   24.    0.4   0.62  0.68  0.9   0.73  0.6   1.95]\n",
      " [24.   48.   24.   48.    0.4  -0.55  0.27  0.9   0.7   0.7   2.2 ]\n",
      " [16.    8.   56.   48.    0.4  -0.44  0.43  0.9   0.7   0.4   1.9 ]\n",
      " [16.   48.   24.   40.    0.2  -0.26  0.48  0.9   1.05  0.6   0.35]\n",
      " [56.   24.   64.   16.    0.2   0.52  0.67  0.9   0.69  0.4   1.95]]\n",
      "Population Results Saved :  EXPERIMENT-1616702854/Updated_PopualtionData_5_TIME1616702856.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "debug = False\n",
    "#Initialize Population  [64, 32, 16, 16, 0.1, P, V, W, loss,accuracy , fitness]\n",
    "popSize = 5\n",
    "for i in range(popSize):\n",
    "    \n",
    "    dropout = round(random.uniform(0.1,0.5), 1)\n",
    "    layer1 = random.randint(1, 8)*8\n",
    "    layer2 = random.randint(1, 8)*8\n",
    "    layer3 = random.randint(1, 8)*8\n",
    "    layer4 = random.randint(1, 8)*8\n",
    "    position = round(random.uniform(0,1), 2)\n",
    "    velocity = round(random.uniform(0,1), 2)\n",
    "    weight= 0.90\n",
    "    \n",
    "    if(debug):\n",
    "        print(\"New Population : \",i+1)\n",
    "       \n",
    "        print(\"layer1: \",layer1, end =\" \")\n",
    "        print(\"layer2: \",layer2, end =\" \")\n",
    "        print(\"layer3: \",layer3, end =\" \")\n",
    "        print(\"layer4: \",layer4, end =\" \")\n",
    "        print(\"dropout: \",dropout, end =\" \")\n",
    "        print(\"velocity: \",velocity, end =\" \")\n",
    "        print(\"position: \",position, end =\" \")\n",
    "        print(\"weight constant: \",weight)\n",
    "    \n",
    "    new = [layer1, layer2, layer3,layer4,dropout,position,velocity,weight,1,0,0]\n",
    "    if(i==0):\n",
    "        popdata = [new]\n",
    "    else:\n",
    "        popdata = np.vstack((popdata, new))\n",
    "    \n",
    "    \n",
    "print(\"Popualtion Shape : \",popdata.shape)\n",
    "print(popdata[1])\n",
    "print(\"Results Acc:\",popdata[1,9])\n",
    "print(\"Results Loss:\",popdata[1,8])\n",
    "\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "d = datetime.utcnow()\n",
    "unixtime = calendar.timegm(d.utctimetuple())\n",
    "#SAVE TO CSV RESULTS\n",
    "\n",
    "csvFileName = path+\"PopualtionData_\"+str(popSize)+\"_TIME\"+str(unixtime)+\".csv\"\n",
    "print(\"Population Results Saved : \",csvFileName )\n",
    "np.savetxt(csvFileName, popdata, delimiter=\",\", fmt='%f')\n",
    "\n",
    "\n",
    "# Update results in tensorflow\n",
    "print (popdata)\n",
    "\n",
    "for i in range(popdata.shape[0]):\n",
    "    \n",
    "    #Runn The Model Here RANDOM NUMBERS FOR NOW\n",
    "    accuracy = round(random.uniform(0.5,0.99), 2)\n",
    "    loss = round(random.uniform(1,0.01), 2)\n",
    "    # get the results\n",
    "    results = RunModelDesign(popdata[i]);\n",
    "    print(\"Validation Loss \",results[0])\n",
    "    print(\"Validation Acc \",results[1])\n",
    "   \n",
    "    popdata[i,8]= round(results[0],2)\n",
    "    popdata[i,9]= round(results[1],2)\n",
    "    print(popdata)\n",
    "    print(\"Saved Progress\",popdata[i])\n",
    "    np.savetxt(csvFileName, popdata, delimiter=\",\", fmt='%f')\n",
    "    if(debug):\n",
    "        print(\"Runnning Model: \",i+1, \" accuracy: \",accuracy,\" loss: \",loss);\n",
    "    \n",
    "print(popdata)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# calculate fitness \n",
    "\n",
    "for i in range(popdata.shape[0]):\n",
    "    accuracy = popdata[i,9]\n",
    "    loss = popdata[i,8]\n",
    "    fitness = accuracy + (1-loss)*5\n",
    "    popdata[i,10]= round(fitness,2)\n",
    "    print(i+1,\"-Calculate Fitness :\",round(fitness,2))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#Find Best model in batch\n",
    "\n",
    "max_index_col = np.argmax(popdata[:,10], axis=0)\n",
    "print(\"Best Model Found\", popdata[max_index_col])\n",
    "global_bestP = popdata[max_index_col,6]\n",
    "print(\" L1     L2     L3    L4    DP    V      P     W    Loss   Acc   Fit\")\n",
    "print(popdata)\n",
    "# calculate new velocity\n",
    "#for each denseLAYER COMBINATION \n",
    "    # V-new = WEIGHT * V-old + C1 * ( RANDOM(0-1) * P) + (RANDOM(0-1) * C2) dense[i,5] (global best P) - Pold \n",
    "    # p-new = P-old + Vnew\n",
    "    #[00, 00,  00, 0.1,Pnew,Vnew,W,loss,accuracy]\n",
    "\n",
    "    \n",
    "    \n",
    "for i in range(popdata.shape[0]):\n",
    "    oldV = popdata[i,5]\n",
    "    oldP = popdata[i,6]  \n",
    "    weight = popdata[i,7]\n",
    "    rand01 = round(random.uniform(0,1), 1)\n",
    "    c1 = 0.1\n",
    "    c2 = 0.1\n",
    "    \n",
    "    newV = weight * oldV +  c1 * ( rand01 * global_bestP ) + (rand01 * c2) - oldP\n",
    "    popdata[i,5] = round(newV,2)\n",
    "    newP = oldP + newV\n",
    "    popdata[i,6] = round(newP,2)\n",
    "print(\"Updates New Position adn Velocity\")\n",
    "print(\" L1     L2     L3    L4    DP    V      P     W    Loss   Acc   Fit\")\n",
    "print(popdata)\n",
    "#each model(on an specific epoch) produces val_loss: 0.6911 min - val_accuracy: 0.5063 max\n",
    "# V-new = WEIGHT * V-old + C1 * RANDOM(0-?)\n",
    "\n",
    "\n",
    "csvFileName = path+\"Updated_PopualtionData_\"+str(popSize)+\"_TIME\"+str(unixtime)+\".csv\"\n",
    "print(\"Population Results Saved : \",csvFileName )\n",
    "np.savetxt(csvFileName, popdata, delimiter=\",\", fmt='%f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
