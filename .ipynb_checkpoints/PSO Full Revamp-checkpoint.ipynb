{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method of data preparation(use this one)\n",
    "assume the data is already given as abnormal and normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.layers import AveragePooling2D,Dropout,Flatten,Dense,Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from imutils import paths\n",
    "from random import shuffle, choice\n",
    "from PIL import Image\n",
    "from random import randint\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading (Fake data for Now ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(60000, 28, 28), y=(60000,)\n",
      "Test: X=(10000, 28, 28), y=(10000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 224, 224, 3)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of loading the mnist dataset\n",
    "from keras.datasets import mnist\n",
    "from matplotlib import pyplot\n",
    "# load dataset\n",
    "(train_data, train_target), (test_data, test_target) = mnist.load_data()\n",
    "# summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (train_data.shape, train_target.shape))\n",
    "print('Test: X=%s, y=%s' % (test_data.shape, test_target.shape))\n",
    "                            \n",
    "train_data = train_data[0:50]                           \n",
    "train_target = train_target[0:50]   \n",
    "test_data = test_data[0:10]   \n",
    "test_target = test_target[0:10]   \n",
    "\n",
    "#Make 0-5 a group as 0  and 5+ as 1 for simplicity\n",
    "test_target = np.where(test_target < 5, 0, 1)  \n",
    "train_target = np.where(train_target < 5, 0, 1)  \n",
    "\n",
    "train_data = np.array(train_data.reshape((train_data.shape[0], 28, 28, 1)))\n",
    "test_data = np.array(test_data.reshape((test_data.shape[0], 28, 28, 1)))\n",
    "\n",
    "train_data =(train_data.repeat(3, -1))  # repeat the last (-1) dimension three times\n",
    "test_data = (test_data.repeat(3, -1))  # repeat the last (-1) dimension three times\n",
    "\n",
    "train_data =(train_data.repeat(8, -2))  # repeat the last (-1) dimension three times\n",
    "test_data = (test_data.repeat(8, -2))  # repeat the last (-1) dimension three times\n",
    "\n",
    "\n",
    "train_data =(train_data.repeat(8, -3))  # repeat the last (-1) dimension three times\n",
    "test_data = (test_data.repeat(8, -3))  # repeat the last (-1) dimension three times\n",
    "\n",
    "tuple(train_target).count(1)\n",
    "tuple(train_target).count(0)\n",
    "\n",
    "\n",
    "tuple(test_target).count(1)\n",
    "tuple(test_target).count(0)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunModelDesign(settings):\n",
    "    \n",
    "  \n",
    "    #Load the VGG19 model, top layers off\n",
    "    baseModel = VGG19(weights=\"imagenet\",include_top=False,input_tensor=Input(shape=(224,224,3)))\n",
    "    INIT_LR= 1e-2 # 0.001 \n",
    "    EPOCHS= 10\n",
    "    BS= 8\n",
    "    \n",
    "    print(\"Running Experiment : \",settings)\n",
    "    print(\"layer-1:\",settings[0])\n",
    "    headModel=baseModel.output\n",
    "    headModel=AveragePooling2D(pool_size=(4,4))(headModel)\n",
    "    headModel=Flatten(name=\"flatten\")(headModel)\n",
    "    headModel=Dense(settings[0],activation=\"relu\")(headModel) #LAYER \n",
    "    headModel=Dense(settings[1],activation=\"relu\")(headModel) #LAYER \n",
    "    headModel=Dense(settings[2],activation=\"relu\")(headModel) #LAYER \n",
    "    headModel=Dense(settings[3],activation=\"relu\")(headModel) #LAYER \n",
    "    headModel=Dropout(settings[4])(headModel)# MIN-MAX 0.1-0.5\n",
    "    headModel=Dense(2,activation=\"softmax\")(headModel) # abnormal /non compliant \n",
    "    model= Model(inputs=baseModel.input, outputs=headModel)\n",
    "    \n",
    "    for layer in baseModel.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    if(debug):\n",
    "        model.summary()\n",
    "        \n",
    "    \n",
    "\n",
    "    print(\"[INFO] compiling model....\")\n",
    "    opt=Adam(lr=INIT_LR,decay=INIT_LR/EPOCHS)\n",
    "    model.compile(loss=\"binary_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])\n",
    "    \n",
    "    print(\"[INFO] Training the model....\")\n",
    "    \n",
    "    \n",
    "    filepath = str(int(settings[0]))+\"-\"+str(int(settings[1]))+\"-\"+str(int(settings[2]))+\"-\"+str(int(settings[3]))+\"-DP0\"+str(int(10*settings[4]))+\"-VGG19.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, \n",
    "                                 save_best_only=True, mode='max')\n",
    "    print(\"[INFO] model file: \",filepath)\n",
    "    \n",
    "    \n",
    "    #initialize training data augmentation \n",
    "    trainAug= ImageDataGenerator(rotation_range=15,\n",
    "                                 rescale=1./255,\n",
    "                                 width_shift_range=0.2,\n",
    "                                 height_shift_range=0.2,\n",
    "                                 shear_range=0.2,\n",
    "                                 zoom_range=0.2,\n",
    "                                 horizontal_flip=True,\n",
    "                                 fill_mode=\"nearest\")\n",
    "    \n",
    "\n",
    "    History=model.fit(\n",
    "        trainAug.flow(train_data,train_target,batch_size=BS),\n",
    "        steps_per_epoch=len(train_data)//BS,\n",
    "        validation_data=(test_data,test_target),\n",
    "        validation_steps=len(test_data)//BS,\n",
    "        epochs=1,\n",
    "        callbacks=[checkpoint])\n",
    "    \n",
    "    num_val_samples = len(test_data)\n",
    "    val_batch_size = BS\n",
    "\n",
    "    val_steps = np.ceil(num_val_samples / val_batch_size) #160 / 8 = 20 steps per epoch\n",
    "    print(len(test_data))\n",
    "\n",
    "        # Here the best epoch will be used.\n",
    "    #validate the model\n",
    "    print(\"[INFO] evaluating model....\")\n",
    "    model.save(filepath)\n",
    "    model.load_weights(filepath)\n",
    "    val_loss, val_acc = model.evaluate(test_data,test_target)\n",
    "    print(test_target)\n",
    "    print('val_loss:', val_loss)\n",
    "    print('val_acc:', val_acc)\n",
    "    \n",
    "    \n",
    "    results = [val_loss,val_acc]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testrun 1 Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Experiment :  [ 8.   56.   24.   16.    0.3   0.74  0.19  0.9   1.    0.    0.  ]\n",
      "layer-1: 8.0\n",
      "[INFO] compiling model....\n",
      "[INFO] Training the model....\n",
      "[INFO] model file:  8-56-24-16-DP03-VGG19.h5\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.5238\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.40000, saving model to 8-56-24-16-DP03-VGG19.h5\n",
      "6/6 [==============================] - 4s 732ms/step - loss: 0.6940 - accuracy: 0.5238 - val_loss: 0.6934 - val_accuracy: 0.4000\n",
      "10\n",
      "[INFO] evaluating model....\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6934 - accuracy: 0.4000\n",
      "[1 0 0 0 0 0 0 1 1 1]\n",
      "val_loss: 0.6934146881103516\n",
      "val_acc: 0.4000000059604645\n",
      "Validation Loss  0.6934146881103516\n",
      "Validation Acc  0.4000000059604645\n"
     ]
    }
   ],
   "source": [
    "results = RunModelDesign(popdata[i])\n",
    "print(\"Validation Loss \",results[0])\n",
    "print(\"Validation Acc \",results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSO LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popualtion Shape :  (5, 11)\n",
      "[24.   24.   24.   16.    0.5   0.33  0.59  0.9   1.    0.    0.  ]\n",
      "Results Acc: 0.0\n",
      "Results Loss: 1.0\n",
      "Population Results Saved :  PopualtionData_5_TIME1616702098.csv\n",
      "[[64.   24.   24.    8.    0.4   0.43  0.23  0.9   1.    0.    0.  ]\n",
      " [24.   24.   24.   16.    0.5   0.33  0.59  0.9   1.    0.    0.  ]\n",
      " [24.   32.   16.   48.    0.2   0.53  0.87  0.9   1.    0.    0.  ]\n",
      " [40.   56.   32.   24.    0.2   0.83  0.72  0.9   1.    0.    0.  ]\n",
      " [40.   48.   16.   48.    0.4   0.06  0.23  0.9   1.    0.    0.  ]]\n",
      "Running Experiment :  [64.   24.   24.    8.    0.4   0.43  0.23  0.9   1.    0.    0.  ]\n",
      "layer-1: 64.0\n",
      "[INFO] compiling model....\n",
      "[INFO] Training the model....\n",
      "[INFO] model file:  64-24-24-8-DP04-VGG19.h5\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6946 - accuracy: 0.4048\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.40000, saving model to 64-24-24-8-DP04-VGG19.h5\n",
      "6/6 [==============================] - 5s 773ms/step - loss: 0.6946 - accuracy: 0.4048 - val_loss: 0.7000 - val_accuracy: 0.4000\n",
      "10\n",
      "[INFO] evaluating model....\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7000 - accuracy: 0.4000\n",
      "[1 0 0 0 0 0 0 1 1 1]\n",
      "val_loss: 0.6999690532684326\n",
      "val_acc: 0.4000000059604645\n",
      "Validation Loss  0.6999690532684326\n",
      "Validation Acc  0.4000000059604645\n",
      "[[64.   24.   24.    8.    0.4   0.43  0.23  0.9   0.7   0.4   0.  ]\n",
      " [24.   24.   24.   16.    0.5   0.33  0.59  0.9   1.    0.    0.  ]\n",
      " [24.   32.   16.   48.    0.2   0.53  0.87  0.9   1.    0.    0.  ]\n",
      " [40.   56.   32.   24.    0.2   0.83  0.72  0.9   1.    0.    0.  ]\n",
      " [40.   48.   16.   48.    0.4   0.06  0.23  0.9   1.    0.    0.  ]]\n",
      "Saved Progress [64.   24.   24.    8.    0.4   0.43  0.23  0.9   0.7   0.4   0.  ]\n",
      "Running Experiment :  [24.   24.   24.   16.    0.5   0.33  0.59  0.9   1.    0.    0.  ]\n",
      "layer-1: 24.0\n",
      "[INFO] compiling model....\n",
      "[INFO] Training the model....\n",
      "[INFO] model file:  24-24-24-16-DP05-VGG19.h5\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.7136 - accuracy: 0.6429\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.30000, saving model to 24-24-24-16-DP05-VGG19.h5\n",
      "6/6 [==============================] - 5s 763ms/step - loss: 0.7136 - accuracy: 0.6429 - val_loss: 0.7343 - val_accuracy: 0.3000\n",
      "10\n",
      "[INFO] evaluating model....\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7343 - accuracy: 0.3000\n",
      "[1 0 0 0 0 0 0 1 1 1]\n",
      "val_loss: 0.734295666217804\n",
      "val_acc: 0.30000001192092896\n",
      "Validation Loss  0.734295666217804\n",
      "Validation Acc  0.30000001192092896\n",
      "[[64.   24.   24.    8.    0.4   0.43  0.23  0.9   0.7   0.4   0.  ]\n",
      " [24.   24.   24.   16.    0.5   0.33  0.59  0.9   0.73  0.3   0.  ]\n",
      " [24.   32.   16.   48.    0.2   0.53  0.87  0.9   1.    0.    0.  ]\n",
      " [40.   56.   32.   24.    0.2   0.83  0.72  0.9   1.    0.    0.  ]\n",
      " [40.   48.   16.   48.    0.4   0.06  0.23  0.9   1.    0.    0.  ]]\n",
      "Saved Progress [24.   24.   24.   16.    0.5   0.33  0.59  0.9   0.73  0.3   0.  ]\n",
      "Running Experiment :  [24.   32.   16.   48.    0.2   0.53  0.87  0.9   1.    0.    0.  ]\n",
      "layer-1: 24.0\n",
      "[INFO] compiling model....\n",
      "[INFO] Training the model....\n",
      "[INFO] model file:  24-32-16-48-DP02-VGG19.h5\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6954 - accuracy: 0.3810\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.40000, saving model to 24-32-16-48-DP02-VGG19.h5\n",
      "6/6 [==============================] - 5s 803ms/step - loss: 0.6954 - accuracy: 0.3810 - val_loss: 0.7483 - val_accuracy: 0.4000\n",
      "10\n",
      "[INFO] evaluating model....\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7483 - accuracy: 0.4000\n",
      "[1 0 0 0 0 0 0 1 1 1]\n",
      "val_loss: 0.7482727766036987\n",
      "val_acc: 0.4000000059604645\n",
      "Validation Loss  0.7482727766036987\n",
      "Validation Acc  0.4000000059604645\n",
      "[[64.   24.   24.    8.    0.4   0.43  0.23  0.9   0.7   0.4   0.  ]\n",
      " [24.   24.   24.   16.    0.5   0.33  0.59  0.9   0.73  0.3   0.  ]\n",
      " [24.   32.   16.   48.    0.2   0.53  0.87  0.9   0.75  0.4   0.  ]\n",
      " [40.   56.   32.   24.    0.2   0.83  0.72  0.9   1.    0.    0.  ]\n",
      " [40.   48.   16.   48.    0.4   0.06  0.23  0.9   1.    0.    0.  ]]\n",
      "Saved Progress [24.   32.   16.   48.    0.2   0.53  0.87  0.9   0.75  0.4   0.  ]\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'PopualtionData_5_TIME1616702098.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-192-91fff6a69a1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpopdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Saved Progress\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpopdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvFileName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpopdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'%f'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Runnning Model: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" accuracy: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" loss: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msavetxt\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[1;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[0;32m   1375\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m         \u001b[1;31m# datasource doesn't support creating a new file ...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1377\u001b[1;33m         \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1378\u001b[0m         \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1379\u001b[0m         \u001b[0mown_fh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'PopualtionData_5_TIME1616702098.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "debug = False\n",
    "#Initialize Population  [64, 32, 16, 16, 0.1, P, V, W, loss,accuracy , fitness]\n",
    "popSize = 5\n",
    "for i in range(popSize):\n",
    "    \n",
    "    dropout = round(random.uniform(0.1,0.5), 1)\n",
    "    layer1 = random.randint(1, 8)*8\n",
    "    layer2 = random.randint(1, 8)*8\n",
    "    layer3 = random.randint(1, 8)*8\n",
    "    layer4 = random.randint(1, 8)*8\n",
    "    position = round(random.uniform(0,1), 2)\n",
    "    velocity = round(random.uniform(0,1), 2)\n",
    "    weight= 0.90\n",
    "    \n",
    "    if(debug):\n",
    "        print(\"New Population : \",i+1)\n",
    "       \n",
    "        print(\"layer1: \",layer1, end =\" \")\n",
    "        print(\"layer2: \",layer2, end =\" \")\n",
    "        print(\"layer3: \",layer3, end =\" \")\n",
    "        print(\"layer4: \",layer4, end =\" \")\n",
    "        print(\"dropout: \",dropout, end =\" \")\n",
    "        print(\"velocity: \",velocity, end =\" \")\n",
    "        print(\"position: \",position, end =\" \")\n",
    "        print(\"weight constant: \",weight)\n",
    "    \n",
    "    new = [layer1, layer2, layer3,layer4,dropout,position,velocity,weight,1,0,0]\n",
    "    if(i==0):\n",
    "        popdata = [new]\n",
    "    else:\n",
    "        popdata = np.vstack((popdata, new))\n",
    "    \n",
    "    \n",
    "print(\"Popualtion Shape : \",popdata.shape)\n",
    "print(popdata[1])\n",
    "print(\"Results Acc:\",popdata[1,9])\n",
    "print(\"Results Loss:\",popdata[1,8])\n",
    "\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "d = datetime.utcnow()\n",
    "unixtime = calendar.timegm(d.utctimetuple())\n",
    "#SAVE TO CSV RESULTS\n",
    "\n",
    "csvFileName = \"PopualtionData_\"+str(popSize)+\"_TIME\"+str(unixtime)+\".csv\"\n",
    "print(\"Population Results Saved : \",csvFileName )\n",
    "np.savetxt(csvFileName, popdata, delimiter=\",\", fmt='%f')\n",
    "\n",
    "\n",
    "# Update results in tensorflow\n",
    "print (popdata)\n",
    "\n",
    "for i in range(popdata.shape[0]):\n",
    "    \n",
    "    #Runn The Model Here RANDOM NUMBERS FOR NOW\n",
    "    accuracy = round(random.uniform(0.5,0.99), 2)\n",
    "    loss = round(random.uniform(1,0.01), 2)\n",
    "    # get the results\n",
    "    results = RunModelDesign(popdata[i]);\n",
    "    print(\"Validation Loss \",results[0])\n",
    "    print(\"Validation Acc \",results[1])\n",
    "   \n",
    "    popdata[i,8]= round(results[0],2)\n",
    "    popdata[i,9]= round(results[1],2)\n",
    "    print(popdata)\n",
    "    print(\"Saved Progress\",popdata[i])\n",
    "    np.savetxt(csvFileName, popdata, delimiter=\",\", fmt='%f')\n",
    "    if(debug):\n",
    "        print(\"Runnning Model: \",i+1, \" accuracy: \",accuracy,\" loss: \",loss);\n",
    "    \n",
    "print(popdata)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# calculate fitness \n",
    "\n",
    "for i in range(popdata.shape[0]):\n",
    "    accuracy = popdata[i,9]\n",
    "    loss = popdata[i,8]\n",
    "    fitness = accuracy + (1-loss)*5\n",
    "    popdata[i,10]= round(fitness,2)\n",
    "    print(i+1,\"-Calculate Fitness :\",round(fitness,2))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#Find Best model in batch\n",
    "\n",
    "max_index_col = np.argmax(popdata[:,10], axis=0)\n",
    "print(\"Best Model Found\", popdata[max_index_col])\n",
    "global_bestP = popdata[max_index_col,6]\n",
    "print(\" L1     L2     L3    L4    DP    V      P     W    Loss   Acc   Fit\")\n",
    "print(popdata)\n",
    "# calculate new velocity\n",
    "#for each denseLAYER COMBINATION \n",
    "    # V-new = WEIGHT * V-old + C1 * ( RANDOM(0-1) * P) + (RANDOM(0-1) * C2) dense[i,5] (global best P) - Pold \n",
    "    # p-new = P-old + Vnew\n",
    "    #[00, 00,  00, 0.1,Pnew,Vnew,W,loss,accuracy]\n",
    "\n",
    "    \n",
    "    \n",
    "for i in range(popdata.shape[0]):\n",
    "    oldV = popdata[i,5]\n",
    "    oldP = popdata[i,6]  \n",
    "    weight = popdata[i,7]\n",
    "    rand01 = round(random.uniform(0,1), 1)\n",
    "    c1 = 0.1\n",
    "    c2 = 0.1\n",
    "    \n",
    "    newV = weight * oldV +  c1 * ( rand01 * global_bestP ) + (rand01 * c2) - oldP\n",
    "    popdata[i,5] = round(newV,2)\n",
    "    newP = oldP + newV\n",
    "    popdata[i,6] = round(newP,2)\n",
    "print(\"Updates New Position adn Velocity\")\n",
    "print(\" L1     L2     L3    L4    DP    V      P     W    Loss   Acc   Fit\")\n",
    "print(popdata)\n",
    "#each model(on an specific epoch) produces val_loss: 0.6911 min - val_accuracy: 0.5063 max\n",
    "# V-new = WEIGHT * V-old + C1 * RANDOM(0-?)\n",
    "\n",
    "\n",
    "csvFileName = \"Updated_PopualtionData_\"+str(popSize)+\"_TIME\"+str(unixtime)+\".csv\"\n",
    "print(\"Population Results Saved : \",csvFileName )\n",
    "np.savetxt(csvFileName, popdata, delimiter=\",\", fmt='%f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
